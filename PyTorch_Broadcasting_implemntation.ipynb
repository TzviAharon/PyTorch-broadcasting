{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this notebook we will implement some broadcasting functions from torch library:\n","\n","* [torch.tensor.expand_as ](https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html)\n","\n","* [torch.broadcast_tensors](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html)\n","\n"],"metadata":{"id":"oc23Zko4bA5r"}},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"y65gbCp_cXe3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Expand_as"],"metadata":{"id":"dk0DA6aPLf3h"}},{"cell_type":"markdown","source":["Initially, we must verify whether, when provided with two tensors A and B, we can resize tensor A to match the dimensions of tensor B.\n","\n","\n","A can be expanded to B if the following conditions are met:\n","1. Both A and B have at least one dimension.\n","2. B has an equal or greater number of dimensions compared to A.\n","3. When iterating through the dimensions of A from the last to the first, each dimension of A is either 1 or matches the corresponding dimension (counting from the end) of B.\n"],"metadata":{"id":"F75Tfkm1bNsV"}},{"cell_type":"code","source":["def has_no_dimensions(tensor):\n","    \"\"\"\n","    Check if the given tensor is None or has no dimensions.\n","\n","    Args:\n","    - tensor (torch.Tensor): The tensor to be checked.\n","\n","    Returns:\n","    - bool: True if the tensor is None or has no dimensions, False otherwise.\n","    \"\"\"\n","\n","    if tensor is None:\n","        return True\n","\n","    if len(tensor.size()) == 0:\n","        return True\n","\n","    return False"],"metadata":{"id":"0oEcW4Gvb115"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_expandable(A, B):\n","  \"\"\"\n","  Check if tensor A is expandable to match the dimensions of tensor B.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor to be checked for expandability.\n","  - B (torch.Tensor): The second tensor whose dimensions are being compared for expansion.\n","\n","  Returns:\n","  - bool: True if A is expandable to B, False otherwise.\n","  \"\"\"\n","\n","  # Obtain the sizes of the tensors\n","  size_A = A.size()\n","  size_B = B.size()\n","\n","\n","  # Check if either tensor is None or has no dimensions\n","  if has_no_dimensions(A) or has_no_dimensions(B):\n","      return False\n","\n","  # Check if B has fewer dimensions than A\n","  if len(size_A) > len(size_B):\n","    return False\n","\n","  # Iterate through the dimensions of A from the trailing dimension\n","  for i in range(-1, -1*len(size_A)-1, -1):\n","\n","    if size_A[i] == 1: # If the dimension of A is 1, it can be expanded\n","      continue\n","\n","    # If the dimensions of A and B at the corresponding index match,\n","    # there is no need to expand\n","    if size_A[i] == size_B[i]:\n","      continue\n","\n","    return False # If one of the conditions isn't met\n","\n","\n","  return True"],"metadata":{"id":"WvoGvA70F85t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Secondly, when presented with a tensor A, we might require augmenting its dimensions by padding until it attains a specific number of dimensions.\n","\n","In our scenario, we may need to pad the dimensions of one tensor to match those of another tensor."],"metadata":{"id":"VnOfe_aubbRH"}},{"cell_type":"code","source":["def dim_pad(A, lenght):\n","  \"\"\"\n","  Pad tensor A to have a specified number of dimensions.\n","\n","  Args:\n","  - A (torch.Tensor): The tensor to be padded.\n","  - length (int): The desired number of dimensions for tensor A.\n","\n","  Returns:\n","  - torch.Tensor: The padded tensor A.\n","\n","  Note:\n","  If tensor A already has a number of dimensions equal to or greater than 'length',\n","  this function won't perform any padding.\n","  \"\"\"\n","\n","  # Create a detached clone of tensor A\n","  A0 = A.clone().detach()\n","\n","  # Pad A to be in number of dimensions = length\n","  size_A = A.size()\n","  if len(size_A) < lenght:\n","    for i in range(lenght - len(size_A)):\n","      A0 = A0.unsqueeze(0)\n","\n","  return A0\n","\n"],"metadata":{"id":"wZ9o1o6-OeWO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If two tensors, A and B, are given, with A being expandable to match B and both having the same number of dimensions, we will extend (separately) each axis of A that does not align with the corresponding axis of B"],"metadata":{"id":"v3FS3xSKbtNc"}},{"cell_type":"code","source":["def expand_along_axis(tensor_in, dim, times):\n","\n","  \"\"\"\n","    Expand tensor along a specified axis by repeating its content without duplicating it in memory (using views of the tensor).\n","\n","    Args:\n","    - tensor_in (torch.Tensor): The input tensor to be expanded.\n","    - dim (int): The axis along which to expand the tensor.\n","    - times (int): The number of times to repeat the tensor's content along the specified axis.\n","\n","    Returns:\n","    - torch.Tensor: The expanded tensor.\n","    \"\"\"\n","\n","  tensor_out = tensor_in.detach().clone()\n","\n","\n","  # Repeat the tensor's content along the specified axis by viewing the other axes multiple times.\n","  for i in range(times-1):\n","      tensor_out = torch.cat([tensor_out, tensor_in.view(tensor_in.size())], dim=dim)\n","\n","  return tensor_out\n"],"metadata":{"id":"w3x0kXAmOkvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If A can be expanded to match B, we'll pad A's dimensions to align with the number of dimensions in B, and we'll expand each axis of A that is equal to 1."],"metadata":{"id":"UIPXylUNctFE"}},{"cell_type":"code","source":["def expand_tensor(input_tensor, *sizes):\n","\n","    \"\"\"\n","    Expand tensor to the specified sizes along the specified dimensions.\n","\n","    Args:\n","    - input_tensor (torch.Tensor): The input tensor to be expanded.\n","    - sizes (int or tuple of ints): The desired sizes along each dimension.\n","\n","    Returns:\n","    - torch.Tensor: The expanded tensor.\n","    \"\"\"\n","\n","    # Initialize output tensor\n","    C = input_tensor.clone()\n","\n","    # Iterate over the desired sizes from the end\n","    for i in range(-1, -1* len(sizes) -1, -1):\n","\n","      # Check if the dimension in the input tensor is 1 and needs expansion\n","      if input_tensor.size()[i] == 1 and sizes[i]!=1:\n","\n","        # Expand along the current dimension\n","        C = expand_along_axis(C, i, sizes[i])\n","\n","\n","    return C\n","\n"],"metadata":{"id":"EsQc3by0YK2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If A can be broadcasted to B, we will adjust their A's sizes by padding it accordinly. Subsequently, we will expand each axis of A that requires expansion."],"metadata":{"id":"zVh0YNhZwoqZ"}},{"cell_type":"code","source":["def my_expand_as(A, B):\n","\n","  size_B = B.size()\n","  # Check if A is expandable to B and perform necessary operations if so\n","  if is_expandable(A,B):\n","    A = dim_pad(A, len(size_B)) # Pad tensor A to match the number of dimensions of tensor B\n","    C = expand_tensor(A, *size_B) # Expand tensor A to match the dimensions of tensor B\n","    return C\n","\n","  else:\n","    raise RuntimeError(\"Can't expand A to B's dimensions\")\n"],"metadata":{"id":"bwgAD39WwalX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare_expand_as(A,B):\n","  \"\"\"\n","  Compare the expansion results of two tensors obtained using my_expand_as and Torch's implementation.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor.\n","  - B (torch.Tensor): The second tensor.\n","\n","  Returns:\n","  - None\n","\n","  Prints:\n","  - str: Message indicating whether the expansion results obtained using custom and Torch's implementations are equal.\n","  \"\"\"\n","\n","  torch_expansion = A.expand_as(B) # Our ground truth\n","  C = my_expand_as(A, B)\n","\n","  if torch.all(C == torch_expansion):\n","      print(\"Expansion results for the provided tensors: my_expand_as matches Torch's implementation.\")\n","  else:\n","    print(\"Expansion results for the provided tensors: my_expand_as differs from Torch's implementation.\")"],"metadata":{"id":"0fq_qFIgxLiE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Usage Example:"],"metadata":{"id":"N-xtHHhce3ct"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vzBjRE1nu9b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8683ec8f-598a-4cbd-d490-5aacd452e907"},"outputs":[{"output_type":"stream","name":"stdout","text":["Expansion results for the provided tensors: my_expand_as matches Torch's implementation.\n","Expansion results for the provided tensors: my_expand_as matches Torch's implementation.\n"]}],"source":["# Examples where A is expandable to B's dimensions\n","\n","D = torch.arange(16).reshape(4,1,4)\n","E = torch.arange(96).reshape(3,4,2,4)\n","compare_expand_as(D,E)\n","\n","F = torch.arange(5).reshape(1, 1, 1, 1, 5)\n","G = torch.arange(160).reshape(2, 2, 2, 2, 2, 5)\n","compare_expand_as(F,G)"]},{"cell_type":"markdown","source":["As observed, there are no discrepancies."],"metadata":{"id":"wVhwWNOQP5me"}},{"cell_type":"code","source":["# Examples where A is not expandable to B's dimensions\n","H = torch.arange(16).reshape(1,4,1,4)\n","I = torch.arange(16).reshape(4,1,4)\n","try:\n","  compare_expand_as(H,I)\n","except RuntimeError as e:\n","  print(e)\n","  print(\"H is expandable to I: \", is_expandable(H,I))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wbr6RXoZy2IW","outputId":"b8c6de17-e7cd-4b04-814e-56d2e4035874"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["expand(torch.LongTensor{[1, 4, 1, 4]}, size=[4, 1, 4]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)\n","H is expandable to I:  False\n"]}]},{"cell_type":"code","source":["J = torch.arange(15).reshape(3, 1, 1, 5)\n","K = torch.arange(40).reshape(2, 2, 2, 5)\n","try:\n","  compare_expand_as(J,K)\n","\n","except RuntimeError as e:\n","  print(e)\n","  print(\"J is expandable to K: \", is_expandable(J,K))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAUZahAd01KF","outputId":"30491338-4df8-4e13-caa8-f88161a825e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [2, 2, 2, 5].  Tensor sizes: [3, 1, 1, 5]\n","J is expandable to K:  False\n"]}]},{"cell_type":"markdown","source":["## broadcast_tensors"],"metadata":{"id":"Z1EpiVr8QFfB"}},{"cell_type":"markdown","source":["\n","Now, our aim is to determine whether two tensors can be broadcasted together. The procedure shares similarities with checking if A is expandable to B but with some variations. The conditions to ascertain whether two tensors, A and B, can be broadcasted together are as follows:\n","\n","1. Both A and B must possess at least one dimension.\n","2. While iterating through the dimensions of one of the tensors from the end to the beginning, each dimension should fulfill one of these three criteria:\n","\n","  a. It is 1.\n","\n","  b. It doesn't exist.\n","\n","  c. It matches the corresponding dimension of the other tensor (counting from the end).\n","\n","\n","In this implementation, we will iterate over the tensors starting with the one having the least dimension. Once we finish iterating through it, there's no need for further checks.\n","\n","Furthermore, during the process of checking for broadcastability, we will determine the appropriate dimension to broadcast the two tensors to (if they are indeed broadcastable).\n"],"metadata":{"id":"adQI0FDAhrut"}},{"cell_type":"code","source":["def is_broadcastable(A, B):\n","  \"\"\"\n","  Check if two tensors are broadcastable together.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor to be checked for broadcastability.\n","  - B (torch.Tensor): The second tensor to be checked for broadcastability.\n","\n","  Returns:\n","  - tuple: A tuple containing:\n","    - bool: True if the tensors are broadcastable, False otherwise.\n","    - list (optional): A list representing the shape of the output tensor after broadcasting,\n","      returned only if the tensors are broadcastable.\n","  \"\"\"\n","\n","\n","  # Check if either tensor is None or has no dimensions\n","  if has_no_dimensions(A) or has_no_dimensions(B):\n","      return False\n","\n","\n","  size_A = A.size()\n","  size_B = B.size()\n","\n","\n","\n","  # Store the dimensions of the shorter and longer tensors (number of dimensions wise) independently.\n","  if len(size_A)<=len(size_B):\n","    shorter_tensor_size = size_A\n","    longer_tensor_size = size_B\n","\n","  else:\n","    shorter_tensor_size  = size_B\n","    longer_tensor_size = size_A\n","\n","\n","\n","  out_dim = []\n","  # Iterate the tensor with least dimensions from the last dimensions to first\n","  for i in range(-1, -1*len(shorter_tensor_size)-1, -1):\n","\n","    # Check if at least one of the tensors in dim=i is 1\n","    if shorter_tensor_size[i] == 1:\n","      out_dim.insert(0, longer_tensor_size[i])\n","\n","    elif longer_tensor_size[i] == 1 :\n","      out_dim.insert(0, shorter_tensor_size[i])\n","\n","    # Check if dimension i in both tensors is equal\n","    elif shorter_tensor_size[i] == longer_tensor_size[i]:\n","      out_dim.insert(0, shorter_tensor_size[i],)\n","\n","    # If both of the conditions before aren't met, the tensors are not broadcastable\n","    else:\n","      return False\n","\n","  # Save the dimensions of the longer tensor that we didn't iterate yet to determine the desired broadcasting size\n","  i-=1\n","  while i>=-1*len(longer_tensor_size):\n","    out_dim.insert(0, longer_tensor_size[i])\n","    i-=1\n","\n","  # There was no violation so the two tensors are broadcastable\n","  return True, out_dim\n","\n","\n"],"metadata":{"id":"_sUzTCohn-NV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once more, there might be a need to pad the dimensions of the tensors. This time, we will pad them to match the number of dimensions of the tensor with the greater number of dimensions."],"metadata":{"id":"I56njErhUnzm"}},{"cell_type":"code","source":["def pad_to_same_size(A, B):\n","\n","  \"\"\"\n","  Pad two tensors to have the same number of dimensions.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor to be padded.\n","  - B (torch.Tensor): The second tensor to be padded.\n","\n","  Returns:\n","  - tuple: A tuple containing the padded tensors:\n","    - torch.Tensor: The padded version of tensor A.\n","    - torch.Tensor: The padded version of tensor B.\n","  \"\"\"\n","\n","  size_A = A.size()\n","  size_B = B.size()\n","\n","  # Pad tensor with less dimensions to match other\n","  if len(size_A)<len(size_B):\n","    A0 = dim_pad(A, len(size_B))\n","    return A0, B\n","\n","  elif len(size_A)>len(size_B):\n","    B0 = dim_pad(B, len(size_A))\n","    return A, B0\n","\n","  else:\n","    return A, B\n"],"metadata":{"id":"oA6G1YZAnP_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, when tensors are broadcastable together and possess an equal number of dimensions, we can perform the broadcasting operation on them simultaneously."],"metadata":{"id":"2mqEqdteU_fi"}},{"cell_type":"code","source":["def broadcast(A, B, target_size):\n","\n","  \"\"\"\n","  Broadcast two tensors to match a target size along each dimension.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor to be broadcasted.\n","  - B (torch.Tensor): The second tensor to be broadcasted.\n","  - target_size (tuple of ints): The desired size along each dimension after broadcasting.\n","\n","  Returns:\n","  - tuple: A tuple containing the broadcasted tensors:\n","    - torch.Tensor: The broadcasted version of tensor A.\n","    - torch.Tensor: The broadcasted version of tensor B.\n","  \"\"\"\n","\n","  A0 = A.clone()\n","  B0 = B.clone()\n","\n","  size_A = A.size()\n","  size_B = B.size()\n","\n","  # Iterate over the target size dimensions from the last to the first\n","  for i in range(-1, -1* len(target_size) -1, -1):\n","\n","    # Broadcast tensor A along the current dimension if needed\n","    if size_A[i] == 1 and target_size[i]!=1:\n","      A0 = expand_along_axis(A0, i, target_size[i])\n","\n","    # Broadcast tensor B along the current dimension if needed\n","    if size_B[i] == 1 and target_size[i]!=1:\n","      B0 = expand_along_axis(B0, i, target_size[i])\n","\n","  return A0, B0\n","\n"],"metadata":{"id":"exNkLkCujNZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","If A and B can be broadcasted together, we will adjust their dimensions' sizes by padding them accordingly. Subsequently, we will expand each axis that requires expansion."],"metadata":{"id":"4EHYNxsNqo2M"}},{"cell_type":"code","source":["def my_broadcast_tensors(A,B):\n","  \"\"\"\n","  Broadcast two tensors to perform element-wise operations.\n","\n","  Args:\n","  - A (torch.Tensor): The first tensor.\n","  - B (torch.Tensor): The second tensor.\n","\n","  Returns:\n","  - tuple: A tuple containing the broadcasted tensors:\n","    - torch.Tensor: The broadcasted version of tensor A.\n","    - torch.Tensor: The broadcasted version of tensor B.\n","\n","  Raises:\n","  - ValueError: If the tensors cannot be broadcasted together.\n","  \"\"\"\n","\n","  # If broadcastable, pad tensors to the same size and broadcast them\n","  broadcast_res = is_broadcastable(A,B)\n","  if broadcast_res:\n","    A, B = pad_to_same_size(A, B)\n","    broadcast_size = broadcast_res[1]\n","    return broadcast(A, B, broadcast_size)\n","\n","  else:\n","    # If not broadcastable, raise a ValueError\n","    raise ValueError('Tried to broadcast two tensors which are not unbroadcastable')\n"],"metadata":{"id":"SMW9ReIQoKm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare_results(X, Y):\n","    \"\"\"\n","    Compare the broadcasting results of two tensors obtained using custom and Torch's implementations.\n","\n","    Args:\n","    - X (torch.Tensor): The first tensor.\n","    - Y (torch.Tensor): The second tensor.\n","\n","    Returns:\n","    - None\n","\n","    Prints:\n","    - str: Message indicating whether the broadcasting results of X obtained using custom and Torch's implementations are equal.\n","    - str: Message indicating whether the broadcasting results of Y obtained using custom and Torch's implementations are equal.\n","\n","    Raises:\n","    - ValueError: If broadcasting is not possible.\n","\n","    Notes:\n","    The function may raise a ValueError if the custom broadcasting implementation in `my_broadcast_tensors` fails.\n","    \"\"\"\n","\n","    # Broadcast tensors X and Y using this my_broadcast_tensors\n","    X0, Y0 = my_broadcast_tensors(X, Y)\n","\n","    # Broadcast tensors X and Y using Torch's implementation\n","    X_t, Y_t = torch.broadcast_tensors(X, Y)\n","\n","    # Compare the broadcasting results of X and Y obtained using my_broadcast_tensors and Torch's implementations\n","    # Print message indicating whether the results are equal or not\n","    if torch.all(X0 == X_t) and torch.all(Y0 == Y_t) :\n","        print(\"Broadcasting results for the provided tensors: my_broadcast_tensors matches Torch's implementation.\")\n","    else:\n","        print(\"Broadcasting results for the provided tensors: my_broadcast_tensors differs from Torch's implementation.\")\n","\n","\n"],"metadata":{"id":"bY9gexrbnl6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Usage examples:"],"metadata":{"id":"qEkSjJqJrNiO"}},{"cell_type":"code","source":["# Broadcastable tensors examples\n","\n","C = torch.arange(24).reshape(2,3,4)\n","D = torch.tensor([0,1,2,3]) # size 4\n","compare_results(C, D) # Compare broadcasting results of tensors A and B\n","\n","\n","R = torch.arange(16).reshape(4,1,4)\n","S = torch.arange(16).reshape(2,1,2,4)\n","compare_results(R, S) # Compare broadcasting results of tensors R and S\n","\n","\n","X = torch.arange(3).reshape(3,1)\n","Y = torch.arange(4).reshape(1,4)\n","compare_results(X, Y) # Compare broadcasting results of tensors X and Y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXAZrBt8qoSo","outputId":"af3194c9-2427-4555-8b1d-ff6bc9899f91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Broadcasting results for the provided tensors: my_broadcast_tensors matches Torch's implementation.\n","Broadcasting results for the provided tensors: my_broadcast_tensors matches Torch's implementation.\n","Broadcasting results for the provided tensors: my_broadcast_tensors matches Torch's implementation.\n"]}]},{"cell_type":"code","source":["# Unbroadcastable tensors example\n","G = torch.arange(6).reshape(3,2)\n","H = torch.arange(5).reshape(1,5)\n","try:\n","  compare_results(G, H) # Compare broadcasting results of tensors A and B\n","\n","except ValueError as e:\n","  print(e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uI340W0sgF2","outputId":"d29caef4-6c45-44a7-bade-35693c5e5987"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tried to broadcast two tensors which are not unbroadcastable\n"]}]},{"cell_type":"code","source":["M = torch.arange(1080).reshape(1,2,3,3,2,5,3,2)\n","N = torch.arange(1620).reshape(3,3,3,2,5,3,2)\n","try:\n","  compare_results(M, N) # Compare broadcasting results of tensors A and B\n","\n","except ValueError as e:\n","  print(e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ctPZBvy2saB","outputId":"42de5b21-7da9-476c-a8f1-464d261df7e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tried to broadcast two tensors which are not unbroadcastable\n"]}]}]}